{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Failed to fetch document: 403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m income_data, line_items \u001b[39m=\u001b[39m parse_income_statement(url)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# To save the data to a file\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mincome_statement_data.npy\u001b[39m\u001b[39m'\u001b[39m, income_data)\n",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to fetch document: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Parse HTML\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch document: 403"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_income_statement(url):\n",
    "    \"\"\"\n",
    "    Parse income statement data from an SEC filing.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to the SEC filing\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Income statement data in a numpy array\n",
    "    \"\"\"\n",
    "    # Fetch the document\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch document: {response.status_code}\")\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the income statement table\n",
    "    # Usually tables with \"Consolidated Statements of Income\" or similar titles\n",
    "    tables = soup.find_all('table')\n",
    "    income_table = None\n",
    "    \n",
    "    for table in tables:\n",
    "        heading_text = \"\"\n",
    "        # Check for table caption or previous heading\n",
    "        caption = table.find('caption')\n",
    "        if caption:\n",
    "            heading_text = caption.get_text().lower()\n",
    "        \n",
    "        # Look for keywords in heading or table text\n",
    "        table_text = table.get_text().lower()\n",
    "        keywords = [\"consolidated statements of income\", \"consolidated income statements\", \n",
    "                   \"statements of operations\", \"income statement\"]\n",
    "        \n",
    "        if any(keyword in heading_text for keyword in keywords) or any(keyword in table_text for keyword in keywords):\n",
    "            income_table = table\n",
    "            break\n",
    "    \n",
    "    if not income_table:\n",
    "        raise Exception(\"Income statement table not found\")\n",
    "    \n",
    "    # Extract rows from the table\n",
    "    rows = income_table.find_all('tr')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    line_items = []\n",
    "    values = []\n",
    "    \n",
    "    # Process rows\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "            \n",
    "        # First cell usually contains the line item description\n",
    "        item = cells[0].get_text().strip()\n",
    "        \n",
    "        # Skip header rows or empty rows\n",
    "        if not item or item.lower() in ['consolidated statements of income', 'in millions, except per share data']:\n",
    "            continue\n",
    "            \n",
    "        # Extract values from other cells, convert to numbers\n",
    "        row_values = []\n",
    "        for cell in cells[1:]:\n",
    "            text = cell.get_text().strip()\n",
    "            \n",
    "            # Handle dollar signs, parentheses (negative values), and commas\n",
    "            if text:\n",
    "                # Remove $ and commas\n",
    "                text = text.replace('$', '').replace(',', '')\n",
    "                \n",
    "                # Handle parentheses for negative values\n",
    "                if '(' in text and ')' in text:\n",
    "                    text = text.replace('(', '-').replace(')', '')\n",
    "                    \n",
    "                try:\n",
    "                    value = float(text)\n",
    "                    row_values.append(value)\n",
    "                except ValueError:\n",
    "                    # If conversion fails, it might be a header or non-numeric cell\n",
    "                    row_values.append(np.nan)\n",
    "            else:\n",
    "                row_values.append(np.nan)\n",
    "                \n",
    "        if row_values and not all(np.isnan(val) for val in row_values):\n",
    "            line_items.append(item)\n",
    "            values.append(row_values)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    if values:\n",
    "        # Find the maximum length of any row\n",
    "        max_length = max(len(row) for row in values)\n",
    "        \n",
    "        # Pad shorter rows with NaN\n",
    "        padded_values = [row + [np.nan] * (max_length - len(row)) for row in values]\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        np_data = np.array(padded_values)\n",
    "        \n",
    "        # Create a structured array with line items as labels\n",
    "        df = pd.DataFrame(np_data, index=line_items)\n",
    "        \n",
    "        # Print the extracted data\n",
    "        print(\"Income Statement Data:\")\n",
    "        print(df)\n",
    "        \n",
    "        return np_data, line_items\n",
    "    else:\n",
    "        raise Exception(\"Failed to extract values from income statement\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\"\n",
    "income_data, line_items = parse_income_statement(url)\n",
    "\n",
    "# To save the data to a file\n",
    "np.save('income_statement_data.npy', income_data)\n",
    "with open('line_items.txt', 'w') as f:\n",
    "    for item in line_items:\n",
    "        f.write(f\"{item}\\n\")\n",
    "\n",
    "print(\"Data saved to income_statement_data.npy and line_items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm#if3830601512b46079053ec0daaf407ac_103\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download the filing. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\"\n",
    "\n",
    "# Set headers to mimic a browser (SEC requires a user-agent)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "}\n",
    "\n",
    "# Send GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the content to a file\n",
    "    with open(\"nvda_filing.html\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Successfully downloaded the filing to 'nvda_filing.html'\")\n",
    "    \n",
    "    # If you want to extract just the specific section from the fragment identifier\n",
    "    # You'll need to parse the HTML and extract that section\n",
    "    # The fragment ID in your URL is: if3830601512b46079053ec0daaf407ac_103\n",
    "else:\n",
    "    print(f\"Failed to download the filing. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sec-edgar-downloader\n",
      "  Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from sec-edgar-downloader) (2.32.3)\n",
      "Collecting pyrate-limiter>=3.6.0 (from sec-edgar-downloader)\n",
      "  Downloading pyrate_limiter-3.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (2025.1.31)\n",
      "Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl (14 kB)\n",
      "Downloading pyrate_limiter-3.7.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: pyrate-limiter, sec-edgar-downloader\n",
      "Successfully installed pyrate-limiter-3.7.0 sec-edgar-downloader-5.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "# Initialize a downloader instance. Download filings to the current\n",
    "# working directory. Must declare company name and email address\n",
    "# to form a user-agent string that complies with the SEC Edgar's\n",
    "# programmatic downloading fair access policy.\n",
    "# More info: https://www.sec.gov/os/webmaster-faq#code-support\n",
    "# Company name and email are used to form a user-agent of the form:\n",
    "# User-Agent: <Company Name> <Email Address>\n",
    "dl = Downloader(\"Analyst Copilot\", \"liamdrew92@icloud.com\")\n",
    "\n",
    "\n",
    "# Get the five most recent 8-K filings for Apple\n",
    "# dl.get(\"8-K\", \"AAPL\", limit=5)\n",
    "\n",
    "# Get the latest 10-K filing for Microsoft\n",
    "dl.get(\"10-K\", \"NVDA\", limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d5/93qfbpbx01l3hnmg99xq3c9c0000gn/T/ipykernel_42040/3473832151.py:20: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  if table.find_previous(text=re.compile(\"NVIDIA Corporation and Subsidiaries Consolidated Statements of Income\", re.IGNORECASE)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find the income statement table in the HTML file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m# # Clean up the DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m# # If we have the right number of headers, use them\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39m# print(\"Len headers is \" + str(len(headers)))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Usage\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m income_statement \u001b[39m=\u001b[39m parse_nvidia_income_statement(\u001b[39m\"\u001b[39;49m\u001b[39msec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mprint\u001b[39m(income_statement)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m# # Save to CSV\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# income_statement.to_csv(\"nvidia_income_statement.csv\")\u001b[39;00m\n",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m income_table:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not find the income statement table in the HTML file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Extract the column headers (years)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# the headers are getting messed up here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m headers \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mItem\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# First column for row names\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find the income statement table in the HTML file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parse_nvidia_income_statement(html_file_path):\n",
    "    # Read the HTML file\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Look for the income statement table\n",
    "    # Search for the table with the title containing \"Consolidated Statements of Income\"\n",
    "    tables = soup.find_all('table')\n",
    "    income_table = None\n",
    "    \n",
    "    for table in tables:\n",
    "        # Check if this table contains the income statement\n",
    "        if table.find_previous(text=re.compile(\"NVIDIA Corporation and Subsidiaries Consolidated Statements of Income\", re.IGNORECASE)):\n",
    "            income_table = table\n",
    "            print(\"Found a table\")\n",
    "            break\n",
    "    \n",
    "    if not income_table:\n",
    "        raise ValueError(\"Could not find the income statement table in the HTML file\")\n",
    "    \n",
    "    # Extract the column headers (years)\n",
    "    # the headers are getting messed up here\n",
    "    headers = [\"Item\"]  # First column for row names\n",
    "\n",
    "    \n",
    "    # Find all header cells\n",
    "    header_cells = income_table.find_all('th')\n",
    "    for cell in header_cells:\n",
    "        text = cell.get_text(strip=True)\n",
    "        if \"Jan\" in text or \"Year Ended\" in text:\n",
    "            headers.append(text)\n",
    "    \n",
    "    # If no headers found in th tags, look in the first row\n",
    "    if len(headers) == 1:  # Only the \"Item\" header we added\n",
    "        first_row = income_table.find('tr')\n",
    "        if first_row:\n",
    "            for cell in first_row.find_all('td'):\n",
    "                text = cell.get_text(strip=True)\n",
    "                if \"Jan\" in text or \"Year Ended\" in text:\n",
    "                    headers.append(text)\n",
    "    \n",
    "    # Create a list to store all rows\n",
    "    data = []\n",
    "    \n",
    "    # Get all rows from the table\n",
    "    rows = income_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        # if len(cells) > 1:  # Skip empty rows\n",
    "        row_data = []\n",
    "        for cell in cells:\n",
    "            text = cell.get_text(strip=False)\n",
    "            # text = cell.get_text(strip=True)\n",
    "            # # Replace empty cells with NaN\n",
    "            # if text == \"\":\n",
    "            #     text = float('nan')\n",
    "            # # Try to convert to numeric if possible\n",
    "            # try:\n",
    "            #     # Remove commas, dollar signs, and parentheses for negative numbers\n",
    "            #     text = text.replace('$', '').replace(',', '')\n",
    "            #     if '(' in text and ')' in text:  # Handle negative numbers in parentheses\n",
    "            #         text = text.replace('(', '-').replace(')', '')\n",
    "            #     text = float(text)\n",
    "            # except (ValueError, AttributeError):\n",
    "            #     pass  # Keep as string if not numeric\n",
    "\n",
    "\n",
    "            row_data.append(text)\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Returning df\")\n",
    "    return df\n",
    "    \n",
    "    # # Clean up the DataFrame\n",
    "    # # If we have the right number of headers, use them\n",
    "    # print(\"Len headers is \" + str(len(headers)))\n",
    "    # print(headers[0])\n",
    "    # print(\"DF shape is \" + str(df.shape[1]))\n",
    "    # if len(headers) == df.shape[1]:\n",
    "    #     # print(\"correct number of headers\")\n",
    "    #     df.columns = headers\n",
    "    # else:\n",
    "    #     # If headers don't match columns, use default naming\n",
    "    #     df.columns = ['Column_' + str(i) for i in range(df.shape[1])]\n",
    "    #     # And put the first row as headers if it looks like headers\n",
    "    #     if any(isinstance(x, str) and \"Revenue\" in x for x in df.iloc[0]):\n",
    "    #         new_columns = df.iloc[0].tolist()\n",
    "    #         df = df[1:]\n",
    "    #         df.columns = new_columns\n",
    "    \n",
    "    # # Set the first column as index if it contains text descriptions\n",
    "    # if df.iloc[:, 0].apply(lambda x: isinstance(x, str)).all():\n",
    "    #     df = df.set_index(df.columns[0])\n",
    "    \n",
    "    # # Clean up the index/row names\n",
    "    # df.index = df.index.str.strip() if hasattr(df.index, 'str') else df.index\n",
    "    \n",
    "    # return df\n",
    "\n",
    "# Usage\n",
    "income_statement = parse_nvidia_income_statement(\"sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\")\n",
    "print(income_statement)\n",
    "\n",
    "# # Save to CSV\n",
    "# income_statement.to_csv(\"nvidia_income_statement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rss-kernel",
   "language": "python",
   "name": "rss-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
