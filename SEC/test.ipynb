{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Failed to fetch document: 403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m income_data, line_items \u001b[39m=\u001b[39m parse_income_statement(url)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# To save the data to a file\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mincome_statement_data.npy\u001b[39m\u001b[39m'\u001b[39m, income_data)\n",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to fetch document: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Parse HTML\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch document: 403"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_income_statement(url):\n",
    "    \"\"\"\n",
    "    Parse income statement data from an SEC filing.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to the SEC filing\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Income statement data in a numpy array\n",
    "    \"\"\"\n",
    "    # Fetch the document\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch document: {response.status_code}\")\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the income statement table\n",
    "    # Usually tables with \"Consolidated Statements of Income\" or similar titles\n",
    "    tables = soup.find_all('table')\n",
    "    income_table = None\n",
    "    \n",
    "    for table in tables:\n",
    "        heading_text = \"\"\n",
    "        # Check for table caption or previous heading\n",
    "        caption = table.find('caption')\n",
    "        if caption:\n",
    "            heading_text = caption.get_text().lower()\n",
    "        \n",
    "        # Look for keywords in heading or table text\n",
    "        table_text = table.get_text().lower()\n",
    "        keywords = [\"consolidated statements of income\", \"consolidated income statements\", \n",
    "                   \"statements of operations\", \"income statement\"]\n",
    "        \n",
    "        if any(keyword in heading_text for keyword in keywords) or any(keyword in table_text for keyword in keywords):\n",
    "            income_table = table\n",
    "            break\n",
    "    \n",
    "    if not income_table:\n",
    "        raise Exception(\"Income statement table not found\")\n",
    "    \n",
    "    # Extract rows from the table\n",
    "    rows = income_table.find_all('tr')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    line_items = []\n",
    "    values = []\n",
    "    \n",
    "    # Process rows\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if len(cells) < 2:\n",
    "            continue\n",
    "            \n",
    "        # First cell usually contains the line item description\n",
    "        item = cells[0].get_text().strip()\n",
    "        \n",
    "        # Skip header rows or empty rows\n",
    "        if not item or item.lower() in ['consolidated statements of income', 'in millions, except per share data']:\n",
    "            continue\n",
    "            \n",
    "        # Extract values from other cells, convert to numbers\n",
    "        row_values = []\n",
    "        for cell in cells[1:]:\n",
    "            text = cell.get_text().strip()\n",
    "            \n",
    "            # Handle dollar signs, parentheses (negative values), and commas\n",
    "            if text:\n",
    "                # Remove $ and commas\n",
    "                text = text.replace('$', '').replace(',', '')\n",
    "                \n",
    "                # Handle parentheses for negative values\n",
    "                if '(' in text and ')' in text:\n",
    "                    text = text.replace('(', '-').replace(')', '')\n",
    "                    \n",
    "                try:\n",
    "                    value = float(text)\n",
    "                    row_values.append(value)\n",
    "                except ValueError:\n",
    "                    # If conversion fails, it might be a header or non-numeric cell\n",
    "                    row_values.append(np.nan)\n",
    "            else:\n",
    "                row_values.append(np.nan)\n",
    "                \n",
    "        if row_values and not all(np.isnan(val) for val in row_values):\n",
    "            line_items.append(item)\n",
    "            values.append(row_values)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    if values:\n",
    "        # Find the maximum length of any row\n",
    "        max_length = max(len(row) for row in values)\n",
    "        \n",
    "        # Pad shorter rows with NaN\n",
    "        padded_values = [row + [np.nan] * (max_length - len(row)) for row in values]\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        np_data = np.array(padded_values)\n",
    "        \n",
    "        # Create a structured array with line items as labels\n",
    "        df = pd.DataFrame(np_data, index=line_items)\n",
    "        \n",
    "        # Print the extracted data\n",
    "        print(\"Income Statement Data:\")\n",
    "        print(df)\n",
    "        \n",
    "        return np_data, line_items\n",
    "    else:\n",
    "        raise Exception(\"Failed to extract values from income statement\")\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\"\n",
    "income_data, line_items = parse_income_statement(url)\n",
    "\n",
    "# To save the data to a file\n",
    "np.save('income_statement_data.npy', income_data)\n",
    "with open('line_items.txt', 'w') as f:\n",
    "    for item in line_items:\n",
    "        f.write(f\"{item}\\n\")\n",
    "\n",
    "print(\"Data saved to income_statement_data.npy and line_items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm#if3830601512b46079053ec0daaf407ac_103\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download the filing. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126.htm\"\n",
    "\n",
    "# Set headers to mimic a browser (SEC requires a user-agent)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "}\n",
    "\n",
    "# Send GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the content to a file\n",
    "    with open(\"nvda_filing.html\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Successfully downloaded the filing to 'nvda_filing.html'\")\n",
    "    \n",
    "    # If you want to extract just the specific section from the fragment identifier\n",
    "    # You'll need to parse the HTML and extract that section\n",
    "    # The fragment ID in your URL is: if3830601512b46079053ec0daaf407ac_103\n",
    "else:\n",
    "    print(f\"Failed to download the filing. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sec-edgar-downloader\n",
      "  Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from sec-edgar-downloader) (2.32.3)\n",
      "Collecting pyrate-limiter>=3.6.0 (from sec-edgar-downloader)\n",
      "  Downloading pyrate_limiter-3.7.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from requests->sec-edgar-downloader) (2025.1.31)\n",
      "Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl (14 kB)\n",
      "Downloading pyrate_limiter-3.7.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: pyrate-limiter, sec-edgar-downloader\n",
      "Successfully installed pyrate-limiter-3.7.0 sec-edgar-downloader-5.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "# Initialize a downloader instance. Download filings to the current\n",
    "# working directory. Must declare company name and email address\n",
    "# to form a user-agent string that complies with the SEC Edgar's\n",
    "# programmatic downloading fair access policy.\n",
    "# More info: https://www.sec.gov/os/webmaster-faq#code-support\n",
    "# Company name and email are used to form a user-agent of the form:\n",
    "# User-Agent: <Company Name> <Email Address>\n",
    "dl = Downloader(\"Analyst Copilot\", \"liamdrew92@icloud.com\")\n",
    "\n",
    "\n",
    "# Get the five most recent 8-K filings for Apple\n",
    "# dl.get(\"8-K\", \"AAPL\", limit=5)\n",
    "\n",
    "# Get the latest 10-K filing for Microsoft\n",
    "dl.get(\"10-K\", \"NVDA\", limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a table\n",
      "Returning df\n",
      "                                   0             1     2             3   \\\n",
      "0                                                                         \n",
      "1                                        Year Ended  None          None   \n",
      "2                                      Jan 26, 2025        Jan 28, 2024   \n",
      "3                             Revenue        100.0      %                 \n",
      "4                     Cost of revenue         25.0                        \n",
      "5                        Gross profit         75.0                        \n",
      "6                  Operating expenses                                     \n",
      "7            Research and development          9.9                        \n",
      "8   Sales, general and administrative          2.7                        \n",
      "9            Total operating expenses         12.6                        \n",
      "10                   Operating income         62.4                        \n",
      "11                    Interest income          1.4                        \n",
      "12                   Interest expense         (0.2)                       \n",
      "13                         Other, net          0.8                        \n",
      "14        Other income (expense), net          2.0                        \n",
      "15           Income before income tax         64.4                        \n",
      "16                 Income tax expense          8.6                        \n",
      "17                         Net income         55.8      %                 \n",
      "\n",
      "        4     5     6     7     8     9     10    11  \n",
      "0                                                     \n",
      "1     None  None  None  None  None  None  None  None  \n",
      "2     None  None  None  None  None  None  None  None  \n",
      "3   100.0      %  None  None  None  None  None  None  \n",
      "4    27.3         None  None  None  None  None  None  \n",
      "5    72.7         None  None  None  None  None  None  \n",
      "6     None  None  None  None  None  None  None  None  \n",
      "7    14.2         None  None  None  None  None  None  \n",
      "8     4.4         None  None  None  None  None  None  \n",
      "9    18.6         None  None  None  None  None  None  \n",
      "10   54.1         None  None  None  None  None  None  \n",
      "11    1.4         None  None  None  None  None  None  \n",
      "12   (0.4)        None  None  None  None  None  None  \n",
      "13    0.4         None  None  None  None  None  None  \n",
      "14    1.4         None  None  None  None  None  None  \n",
      "15   55.5         None  None  None  None  None  None  \n",
      "16    6.6         None  None  None  None  None  None  \n",
      "17   48.9      %  None  None  None  None  None  None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d5/93qfbpbx01l3hnmg99xq3c9c0000gn/T/ipykernel_1950/2234395503.py:20: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  if table.find_previous(text=re.compile(\"Consolidated Statements of Income\", re.IGNORECASE)):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parse_nvidia_income_statement(html_file_path):\n",
    "    # Read the HTML file\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Look for the income statement table\n",
    "    # Search for the table with the title containing \"Consolidated Statements of Income\"\n",
    "    tables = soup.find_all('table')\n",
    "    income_table = None\n",
    "    \n",
    "    for table in tables:\n",
    "        # Check if this table contains the income statement\n",
    "        if table.find_previous(text=re.compile(\"Consolidated Statements of Income\", re.IGNORECASE)):\n",
    "            income_table = table\n",
    "            print(\"Found a table\")\n",
    "            break\n",
    "    \n",
    "    if not income_table:\n",
    "        raise ValueError(\"Could not find the income statement table in the HTML file\")\n",
    "    \n",
    "    # Extract the column headers (years)\n",
    "    # the headers are getting messed up here\n",
    "    headers = [\"Item\"]  # First column for row names\n",
    "\n",
    "    \n",
    "    # Find all header cells\n",
    "    header_cells = income_table.find_all('th')\n",
    "    for cell in header_cells:\n",
    "        text = cell.get_text(strip=True)\n",
    "        if \"Jan\" in text or \"Year Ended\" in text:\n",
    "            headers.append(text)\n",
    "    \n",
    "    # If no headers found in th tags, look in the first row\n",
    "    if len(headers) == 1:  # Only the \"Item\" header we added\n",
    "        first_row = income_table.find('tr')\n",
    "        if first_row:\n",
    "            for cell in first_row.find_all('td'):\n",
    "                text = cell.get_text(strip=True)\n",
    "                if \"Jan\" in text or \"Year Ended\" in text:\n",
    "                    headers.append(text)\n",
    "    \n",
    "    # Create a list to store all rows\n",
    "    data = []\n",
    "    \n",
    "    # Get all rows from the table\n",
    "    rows = income_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        # if len(cells) > 1:  # Skip empty rows\n",
    "        row_data = []\n",
    "        for cell in cells:\n",
    "            text = cell.get_text(strip=False)\n",
    "            # text = cell.get_text(strip=True)\n",
    "            # # Replace empty cells with NaN\n",
    "            # if text == \"\":\n",
    "            #     text = float('nan')\n",
    "            # # Try to convert to numeric if possible\n",
    "            # try:\n",
    "            #     # Remove commas, dollar signs, and parentheses for negative numbers\n",
    "            #     text = text.replace('$', '').replace(',', '')\n",
    "            #     if '(' in text and ')' in text:  # Handle negative numbers in parentheses\n",
    "            #         text = text.replace('(', '-').replace(')', '')\n",
    "            #     text = float(text)\n",
    "            # except (ValueError, AttributeError):\n",
    "            #     pass  # Keep as string if not numeric\n",
    "\n",
    "\n",
    "            row_data.append(text)\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Returning df\")\n",
    "    return df\n",
    "    \n",
    "    # # Clean up the DataFrame\n",
    "    # # If we have the right number of headers, use them\n",
    "    # print(\"Len headers is \" + str(len(headers)))\n",
    "    # print(headers[0])\n",
    "    # print(\"DF shape is \" + str(df.shape[1]))\n",
    "    # if len(headers) == df.shape[1]:\n",
    "    #     # print(\"correct number of headers\")\n",
    "    #     df.columns = headers\n",
    "    # else:\n",
    "    #     # If headers don't match columns, use default naming\n",
    "    #     df.columns = ['Column_' + str(i) for i in range(df.shape[1])]\n",
    "    #     # And put the first row as headers if it looks like headers\n",
    "    #     if any(isinstance(x, str) and \"Revenue\" in x for x in df.iloc[0]):\n",
    "    #         new_columns = df.iloc[0].tolist()\n",
    "    #         df = df[1:]\n",
    "    #         df.columns = new_columns\n",
    "    \n",
    "    # # Set the first column as index if it contains text descriptions\n",
    "    # if df.iloc[:, 0].apply(lambda x: isinstance(x, str)).all():\n",
    "    #     df = df.set_index(df.columns[0])\n",
    "    \n",
    "    # # Clean up the index/row names\n",
    "    # df.index = df.index.str.strip() if hasattr(df.index, 'str') else df.index\n",
    "    \n",
    "    # return df\n",
    "\n",
    "# Usage\n",
    "income_statement = parse_nvidia_income_statement(\"sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\")\n",
    "print(income_statement)\n",
    "\n",
    "# # Save to CSV\n",
    "# income_statement.to_csv(\"nvidia_income_statement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'company_10k.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     \u001b[39m# Option 1: Load from URL\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     \u001b[39m# url = \"https://example.com/company-10k.html\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     \u001b[39m# html_content = load_10k_from_url(url)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m     \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m     \u001b[39m# Option 2: Load from local file\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompany_10k.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m     html_content \u001b[39m=\u001b[39m load_10k_from_file(file_path)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m     \u001b[39m# Parse the 10-K\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m     parsed_10k \u001b[39m=\u001b[39m parse_10k(html_content)\n",
      "\u001b[1;32m/Users/liam/Development/analyst_copilot/SEC/test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_10k_from_file\u001b[39m(file_path):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39m    Load a 10-K document from a local file.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m        str: The HTML content of the 10-K document\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liam/Development/analyst_copilot/SEC/test.ipynb#X12sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'company_10k.html'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def parse_10k(html_content):\n",
    "    \"\"\"\n",
    "    Parse a 10-K HTML document and extract sections organized by Parts and Items.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The HTML content of the 10-K document\n",
    "        \n",
    "    Returns:\n",
    "        dict: A nested dictionary with Parts and Items from the 10-K\n",
    "    \"\"\"\n",
    "    # Create BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Dictionary to store the parsed results\n",
    "    parsed_10k = {}\n",
    "    \n",
    "    # Regular expressions for matching Parts and Items\n",
    "    part_pattern = re.compile(r'\\s*PART\\s+([IVX]+)\\s*', re.IGNORECASE)\n",
    "    item_pattern = re.compile(r'\\s*Item\\s+(\\d+[A-Z]?)\\.?\\s*(.*?)(?:\\s*\\d+\\s*)?$', re.IGNORECASE)\n",
    "    \n",
    "    # Find all headings that might contain Parts or Items\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'div', 'p', 'span'], \n",
    "                             class_=lambda c: c and ('title' in c.lower() or 'heading' in c.lower() or 'header' in c.lower()))\n",
    "    \n",
    "    # Add all strong and b tags which might also contain headers\n",
    "    headings.extend(soup.find_all(['strong', 'b']))\n",
    "    \n",
    "    current_part = None\n",
    "    current_item = None\n",
    "    \n",
    "    # Process all potential headings\n",
    "    for heading in headings:\n",
    "        text = heading.get_text().strip()\n",
    "        \n",
    "        # Check if it's a Part\n",
    "        part_match = part_pattern.match(text)\n",
    "        if part_match:\n",
    "            current_part = f\"Part {part_match.group(1)}\"\n",
    "            parsed_10k[current_part] = {}\n",
    "            continue\n",
    "            \n",
    "        # Check if it's an Item\n",
    "        item_match = item_pattern.match(text)\n",
    "        if item_match and current_part:\n",
    "            item_num = item_match.group(1)\n",
    "            item_title = item_match.group(2).strip()\n",
    "            current_item = f\"Item {item_num}\"\n",
    "            \n",
    "            # Create the item entry in the current part\n",
    "            parsed_10k[current_part][current_item] = {\n",
    "                'title': item_title,\n",
    "                'content': '',  # We'll extract content in a second pass\n",
    "                'page': None\n",
    "            }\n",
    "            \n",
    "            # Try to find the page number that might follow the item title\n",
    "            page_match = re.search(r'\\d+$', text)\n",
    "            if page_match:\n",
    "                parsed_10k[current_part][current_item]['page'] = int(page_match.group())\n",
    "    \n",
    "    # Extract content for each item\n",
    "    extract_item_content(soup, parsed_10k)\n",
    "    \n",
    "    return parsed_10k\n",
    "\n",
    "def extract_item_content(soup, parsed_10k):\n",
    "    \"\"\"\n",
    "    Extract content for each item in the 10-K document.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the HTML document\n",
    "        parsed_10k (dict): The dictionary containing the structure of the 10-K\n",
    "    \"\"\"\n",
    "    # Get all parts and items in order\n",
    "    all_parts = list(parsed_10k.keys())\n",
    "    \n",
    "    # For each part, process its items\n",
    "    for i, part in enumerate(all_parts):\n",
    "        all_items = list(parsed_10k[part].keys())\n",
    "        \n",
    "        # For each item, find its content until the next item or part\n",
    "        for j, item in enumerate(all_items):\n",
    "            # Find the element that contains this item\n",
    "            item_text = f\"{item}. {parsed_10k[part][item]['title']}\"\n",
    "            item_elements = soup.find_all(text=lambda text: text and item_text in text)\n",
    "            \n",
    "            if not item_elements:\n",
    "                # Try with a more flexible search\n",
    "                item_elements = soup.find_all(text=lambda text: text and item.lower() in text.lower() and parsed_10k[part][item]['title'].lower() in text.lower())\n",
    "            \n",
    "            if item_elements:\n",
    "                current_element = item_elements[0].parent\n",
    "                content = []\n",
    "                \n",
    "                # Determine where to stop extracting content\n",
    "                next_item = all_items[j+1] if j+1 < len(all_items) else None\n",
    "                next_part = all_parts[i+1] if i+1 < len(all_parts) else None\n",
    "                \n",
    "                # Extract content until next item or part is found\n",
    "                while current_element and current_element.next_sibling:\n",
    "                    current_element = current_element.next_sibling\n",
    "                    \n",
    "                    # Check if we've reached the next item or part\n",
    "                    elem_text = current_element.get_text().strip() if hasattr(current_element, 'get_text') else \"\"\n",
    "                    \n",
    "                    if next_item and next_item in elem_text:\n",
    "                        break\n",
    "                    if next_part and next_part in elem_text:\n",
    "                        break\n",
    "                    \n",
    "                    if hasattr(current_element, 'get_text'):\n",
    "                        content.append(current_element.get_text().strip())\n",
    "                \n",
    "                parsed_10k[part][item]['content'] = \"\\n\".join(content)\n",
    "\n",
    "def load_10k_from_url(url):\n",
    "    \"\"\"\n",
    "    Load a 10-K document from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the 10-K document\n",
    "        \n",
    "    Returns:\n",
    "        str: The HTML content of the 10-K document\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(f\"Failed to load document from {url}. Status code: {response.status_code}\")\n",
    "\n",
    "def load_10k_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a 10-K document from a local file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the local 10-K HTML file\n",
    "        \n",
    "    Returns:\n",
    "        str: The HTML content of the 10-K document\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_to_csv(parsed_10k, output_file):\n",
    "    \"\"\"\n",
    "    Save the parsed 10-K to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        parsed_10k (dict): The parsed 10-K document\n",
    "        output_file (str): The path to save the CSV file\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for part, items in parsed_10k.items():\n",
    "        for item, details in items.items():\n",
    "            rows.append({\n",
    "                'Part': part,\n",
    "                'Item': item,\n",
    "                'Title': details['title'],\n",
    "                'Page': details['page'],\n",
    "                'Content': details['content']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved parsed 10-K to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Load from URL\n",
    "    # url = \"https://example.com/company-10k.html\"\n",
    "    # html_content = load_10k_from_url(url)\n",
    "    \n",
    "    # Option 2: Load from local file\n",
    "    file_path = \"company_10k.html\"\n",
    "    html_content = load_10k_from_file(file_path)\n",
    "    \n",
    "    # Parse the 10-K\n",
    "    parsed_10k = parse_10k(html_content)\n",
    "    \n",
    "    # Print the structure\n",
    "    for part, items in parsed_10k.items():\n",
    "        print(f\"\\n{part}\")\n",
    "        for item, details in items.items():\n",
    "            print(f\"  {item}: {details['title']} (Page {details['page']})\")\n",
    "            # Uncomment to print content snippets\n",
    "            # content_preview = details['content'][:100] + '...' if details['content'] else 'No content extracted'\n",
    "            # print(f\"    {content_preview}\")\n",
    "    \n",
    "    \n",
    "    # Save to CSV\n",
    "    save_to_csv(parsed_10k, \"parsed_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded HTML file from sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\n",
      "Saved parsed 10-K to nvidia_10k_parsed.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def parse_10k(html_content):\n",
    "    \"\"\"\n",
    "    Parse a 10-K HTML document and extract sections organized by Parts and Items.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The HTML content of the 10-K document\n",
    "        \n",
    "    Returns:\n",
    "        dict: A nested dictionary with Parts and Items from the 10-K\n",
    "    \"\"\"\n",
    "    # Create BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Dictionary to store the parsed results\n",
    "    parsed_10k = {}\n",
    "    \n",
    "    # Regular expressions for matching Parts and Items\n",
    "    part_pattern = re.compile(r'\\s*PART\\s+([IVX]+)\\s*', re.IGNORECASE)\n",
    "    item_pattern = re.compile(r'\\s*Item\\s+(\\d+[A-Z]?)\\.?\\s*(.*?)(?:\\s*\\d+\\s*)?$', re.IGNORECASE)\n",
    "    \n",
    "    # Find all headings that might contain Parts or Items\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'div', 'p', 'span'], \n",
    "                             class_=lambda c: c and ('title' in c.lower() or 'heading' in c.lower() or 'header' in c.lower()))\n",
    "    \n",
    "    # Add all strong and b tags which might also contain headers\n",
    "    headings.extend(soup.find_all(['strong', 'b']))\n",
    "    \n",
    "    current_part = None\n",
    "    current_item = None\n",
    "    \n",
    "    # Process all potential headings\n",
    "    for heading in headings:\n",
    "        text = heading.get_text().strip()\n",
    "        \n",
    "        # Check if it's a Part\n",
    "        part_match = part_pattern.match(text)\n",
    "        if part_match:\n",
    "            current_part = f\"Part {part_match.group(1)}\"\n",
    "            parsed_10k[current_part] = {}\n",
    "            continue\n",
    "            \n",
    "        # Check if it's an Item\n",
    "        item_match = item_pattern.match(text)\n",
    "        if item_match and current_part:\n",
    "            item_num = item_match.group(1)\n",
    "            item_title = item_match.group(2).strip()\n",
    "            current_item = f\"Item {item_num}\"\n",
    "            \n",
    "            # Create the item entry in the current part\n",
    "            parsed_10k[current_part][current_item] = {\n",
    "                'title': item_title,\n",
    "                'content': '',  # We'll extract content in a second pass\n",
    "                'page': None\n",
    "            }\n",
    "            \n",
    "            # Try to find the page number that might follow the item title\n",
    "            page_match = re.search(r'\\d+$', text)\n",
    "            if page_match:\n",
    "                parsed_10k[current_part][current_item]['page'] = int(page_match.group())\n",
    "    \n",
    "    # Extract content for each item\n",
    "    extract_item_content(soup, parsed_10k)\n",
    "    \n",
    "    return parsed_10k\n",
    "\n",
    "def extract_item_content(soup, parsed_10k):\n",
    "    \"\"\"\n",
    "    Extract content for each item in the 10-K document.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the HTML document\n",
    "        parsed_10k (dict): The dictionary containing the structure of the 10-K\n",
    "    \"\"\"\n",
    "    # Get all parts and items in order\n",
    "    all_parts = list(parsed_10k.keys())\n",
    "    \n",
    "    # For each part, process its items\n",
    "    for i, part in enumerate(all_parts):\n",
    "        all_items = list(parsed_10k[part].keys())\n",
    "        \n",
    "        # For each item, find its content until the next item or part\n",
    "        for j, item in enumerate(all_items):\n",
    "            # Find the element that contains this item\n",
    "            item_text = f\"{item}. {parsed_10k[part][item]['title']}\"\n",
    "            item_elements = soup.find_all(text=lambda text: text and item_text in text)\n",
    "            \n",
    "            if not item_elements:\n",
    "                # Try with a more flexible search\n",
    "                item_elements = soup.find_all(text=lambda text: text and item.lower() in text.lower() and parsed_10k[part][item]['title'].lower() in text.lower())\n",
    "            \n",
    "            if item_elements:\n",
    "                current_element = item_elements[0].parent\n",
    "                content = []\n",
    "                \n",
    "                # Determine where to stop extracting content\n",
    "                next_item = all_items[j+1] if j+1 < len(all_items) else None\n",
    "                next_part = all_parts[i+1] if i+1 < len(all_parts) else None\n",
    "                \n",
    "                # Extract content until next item or part is found\n",
    "                while current_element and current_element.next_sibling:\n",
    "                    current_element = current_element.next_sibling\n",
    "                    \n",
    "                    # Check if we've reached the next item or part\n",
    "                    elem_text = current_element.get_text().strip() if hasattr(current_element, 'get_text') else \"\"\n",
    "                    \n",
    "                    if next_item and next_item in elem_text:\n",
    "                        break\n",
    "                    if next_part and next_part in elem_text:\n",
    "                        break\n",
    "                    \n",
    "                    if hasattr(current_element, 'get_text'):\n",
    "                        content.append(current_element.get_text().strip())\n",
    "                \n",
    "                parsed_10k[part][item]['content'] = \"\\n\".join(content)\n",
    "\n",
    "def load_10k_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a 10-K document from a local file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the local 10-K HTML file\n",
    "        \n",
    "    Returns:\n",
    "        str: The HTML content of the 10-K document\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_to_csv(parsed_10k, output_file):\n",
    "    \"\"\"\n",
    "    Save the parsed 10-K to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        parsed_10k (dict): The parsed 10-K document\n",
    "        output_file (str): The path to save the CSV file\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for part, items in parsed_10k.items():\n",
    "        for item, details in items.items():\n",
    "            rows.append({\n",
    "                'Part': part,\n",
    "                'Item': item,\n",
    "                'Title': details['title'],\n",
    "                'Page': details['page'],\n",
    "                'Content': details['content']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved parsed 10-K to {output_file}\")\n",
    "\n",
    "# Use your specific file path\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\"\n",
    "    \n",
    "    try:\n",
    "        html_content = load_10k_from_file(file_path)\n",
    "        print(f\"Successfully loaded HTML file from {file_path}\")\n",
    "        \n",
    "        # Parse the 10-K\n",
    "        parsed_10k = parse_10k(html_content)\n",
    "        \n",
    "        # Print the structure\n",
    "        for part, items in parsed_10k.items():\n",
    "            print(f\"\\n{part}\")\n",
    "            for item, details in items.items():\n",
    "                print(f\"  {item}: {details['title']} (Page {details['page']})\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = \"nvidia_10k_parsed.csv\"\n",
    "        save_to_csv(parsed_10k, output_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded HTML file from sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\n",
      "Saved parsed 10-K to nvidia_10k_parsed.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def parse_10k(html_content):\n",
    "    \"\"\"\n",
    "    Parse a 10-K HTML document and extract sections organized by Parts and Items.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The HTML content of the 10-K document\n",
    "        \n",
    "    Returns:\n",
    "        dict: A nested dictionary with Parts and Items from the 10-K\n",
    "    \"\"\"\n",
    "    # Create BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Dictionary to store the parsed results\n",
    "    parsed_10k = {}\n",
    "    \n",
    "    # Regular expressions for matching Parts and Items\n",
    "    part_pattern = re.compile(r'\\s*PART\\s+([IVX]+)\\s*', re.IGNORECASE)\n",
    "    item_pattern = re.compile(r'\\s*Item\\s+(\\d+[A-Z]?)\\.?\\s*(.*?)(?:\\s*\\d+\\s*)?$', re.IGNORECASE)\n",
    "    \n",
    "    # Find all headings that might contain Parts or Items\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'div', 'p', 'span'], \n",
    "                             class_=lambda c: c and ('title' in c.lower() or 'heading' in c.lower() or 'header' in c.lower()))\n",
    "    \n",
    "    # Add all strong and b tags which might also contain headers\n",
    "    headings.extend(soup.find_all(['strong', 'b']))\n",
    "    \n",
    "    current_part = None\n",
    "    current_item = None\n",
    "    \n",
    "    # Process all potential headings\n",
    "    for heading in headings:\n",
    "        text = heading.get_text().strip()\n",
    "        \n",
    "        # Check if it's a Part\n",
    "        part_match = part_pattern.match(text)\n",
    "        if part_match:\n",
    "            current_part = f\"Part {part_match.group(1)}\"\n",
    "            parsed_10k[current_part] = {}\n",
    "            continue\n",
    "            \n",
    "        # Check if it's an Item\n",
    "        item_match = item_pattern.match(text)\n",
    "        if item_match and current_part:\n",
    "            item_num = item_match.group(1)\n",
    "            item_title = item_match.group(2).strip()\n",
    "            current_item = f\"Item {item_num}\"\n",
    "            \n",
    "            # Create the item entry in the current part\n",
    "            parsed_10k[current_part][current_item] = {\n",
    "                'title': item_title,\n",
    "                'content': '',  # We'll extract content in a second pass\n",
    "                'page': None\n",
    "            }\n",
    "            \n",
    "            # Try to find the page number that might follow the item title\n",
    "            page_match = re.search(r'\\d+$', text)\n",
    "            if page_match:\n",
    "                parsed_10k[current_part][current_item]['page'] = int(page_match.group())\n",
    "    \n",
    "    # Extract content for each item\n",
    "    extract_item_content(soup, parsed_10k)\n",
    "    \n",
    "    return parsed_10k\n",
    "\n",
    "def extract_item_content(soup, parsed_10k):\n",
    "    \"\"\"\n",
    "    Extract content for each item in the 10-K document.\n",
    "    \n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object of the HTML document\n",
    "        parsed_10k (dict): The dictionary containing the structure of the 10-K\n",
    "    \"\"\"\n",
    "    # Get all parts and items in order\n",
    "    all_parts = list(parsed_10k.keys())\n",
    "    \n",
    "    # For each part, process its items\n",
    "    for i, part in enumerate(all_parts):\n",
    "        all_items = list(parsed_10k[part].keys())\n",
    "        \n",
    "        # For each item, find its content until the next item or part\n",
    "        for j, item in enumerate(all_items):\n",
    "            # Find the element that contains this item\n",
    "            item_text = f\"{item}. {parsed_10k[part][item]['title']}\"\n",
    "            item_elements = soup.find_all(text=lambda text: text and item_text in text)\n",
    "            \n",
    "            if not item_elements:\n",
    "                # Try with a more flexible search\n",
    "                item_elements = soup.find_all(text=lambda text: text and item.lower() in text.lower() and parsed_10k[part][item]['title'].lower() in text.lower())\n",
    "            \n",
    "            if item_elements:\n",
    "                current_element = item_elements[0].parent\n",
    "                content = []\n",
    "                \n",
    "                # Determine where to stop extracting content\n",
    "                next_item = all_items[j+1] if j+1 < len(all_items) else None\n",
    "                next_part = all_parts[i+1] if i+1 < len(all_parts) else None\n",
    "                \n",
    "                # Extract content until next item or part is found\n",
    "                while current_element and current_element.next_sibling:\n",
    "                    current_element = current_element.next_sibling\n",
    "                    \n",
    "                    # Check if we've reached the next item or part\n",
    "                    elem_text = current_element.get_text().strip() if hasattr(current_element, 'get_text') else \"\"\n",
    "                    \n",
    "                    if next_item and next_item in elem_text:\n",
    "                        break\n",
    "                    if next_part and next_part in elem_text:\n",
    "                        break\n",
    "                    \n",
    "                    if hasattr(current_element, 'get_text'):\n",
    "                        content.append(current_element.get_text().strip())\n",
    "                \n",
    "                parsed_10k[part][item]['content'] = \"\\n\".join(content)\n",
    "\n",
    "def load_10k_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a 10-K document from a local file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the local 10-K HTML file\n",
    "        \n",
    "    Returns:\n",
    "        str: The HTML content of the 10-K document\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_to_csv(parsed_10k, output_file):\n",
    "    \"\"\"\n",
    "    Save the parsed 10-K to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        parsed_10k (dict): The parsed 10-K document\n",
    "        output_file (str): The path to save the CSV file\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for part, items in parsed_10k.items():\n",
    "        for item, details in items.items():\n",
    "            rows.append({\n",
    "                'Part': part,\n",
    "                'Item': item,\n",
    "                'Title': details['title'],\n",
    "                'Page': details['page'],\n",
    "                'Content': details['content']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved parsed 10-K to {output_file}\")\n",
    "\n",
    "# Use your specific file path\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"sec-edgar-filings/NVDA/10-K/0001045810-25-000023/full-submission.html\"\n",
    "    \n",
    "    try:\n",
    "        html_content = load_10k_from_file(file_path)\n",
    "        print(f\"Successfully loaded HTML file from {file_path}\")\n",
    "        \n",
    "        # Parse the 10-K\n",
    "        parsed_10k = parse_10k(html_content)\n",
    "        \n",
    "        # Print the structure\n",
    "        for part, items in parsed_10k.items():\n",
    "            print(f\"\\n{part}\")\n",
    "            for item, details in items.items():\n",
    "                print(f\"  {item}: {details['title']} (Page {details['page']})\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = \"nvidia_10k_parsed.csv\"\n",
    "        save_to_csv(parsed_10k, output_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/liam/Development/analyst_copilot/RSS/rss-env/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rss-kernel",
   "language": "python",
   "name": "rss-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
