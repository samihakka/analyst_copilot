{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1245660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import datetime\n",
    "import csv\n",
    "from typing import List, Dict, Optional\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import socket\n",
    "import time\n",
    "\n",
    "class PRWireParser:\n",
    "    def __init__(self, feed_url: str, timeout: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the PR Wire parser with a feed URL.\n",
    "        \n",
    "        Args:\n",
    "            feed_url (str): The URL of the RSS feed to parse\n",
    "            timeout (int): Timeout in seconds for feed requests\n",
    "        \"\"\"\n",
    "        self.feed_url = feed_url\n",
    "        self.timeout = timeout\n",
    "        self.feed_data = None\n",
    "        \n",
    "        # Set socket timeout globally\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "\n",
    "    def fetch_feed(self) -> bool:\n",
    "        \"\"\"\n",
    "        Fetch and parse the RSS feed with timeout handling.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # First get the raw feed content with timeout\n",
    "            response = requests.get(self.feed_url, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the feed content\n",
    "            self.feed_data = feedparser.parse(response.content)\n",
    "            \n",
    "            # Check if parsing was successful\n",
    "            if not hasattr(self.feed_data, 'entries'):\n",
    "                print(f\"No entries found in feed: {self.feed_url}\")\n",
    "                return False\n",
    "                \n",
    "            print(f\"Feed fetched in {time.time() - start_time:.2f} seconds\")\n",
    "            return len(self.feed_data.entries) > 0\n",
    "            \n",
    "        except RequestException as e:\n",
    "            print(f\"Request error fetching feed {self.feed_url}: {str(e)}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing feed {self.feed_url}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_entries(self, limit: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get parsed entries from the feed with performance optimization.\n",
    "        \n",
    "        Args:\n",
    "            limit (Optional[int]): Maximum number of entries to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of parsed entries\n",
    "        \"\"\"\n",
    "        if not self.feed_data:\n",
    "            if not self.fetch_feed():\n",
    "                return []\n",
    "\n",
    "        entries = []\n",
    "        try:\n",
    "            # Limit the number of entries to process\n",
    "            feed_entries = self.feed_data.entries[:limit] if limit else self.feed_data.entries\n",
    "            \n",
    "            for entry in feed_entries:\n",
    "                # Skip entries that don't have required fields\n",
    "                if not entry.get('title') or not entry.get('link'):\n",
    "                    continue\n",
    "                    \n",
    "                parsed_entry = {\n",
    "                    'title': entry.get('title', ''),\n",
    "                    'link': entry.get('link', ''),\n",
    "                    'published': entry.get('published', ''),\n",
    "                    'summary': entry.get('summary', '')[:500],  # Limit summary length\n",
    "                    'company': self._extract_company(entry),\n",
    "                    'categories': ','.join([tag.term for tag in entry.get('tags', [])])\n",
    "                }\n",
    "                entries.append(parsed_entry)\n",
    "                \n",
    "                # Early exit if we've reached the limit\n",
    "                if limit and len(entries) >= limit:\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entries: {str(e)}\")\n",
    "            \n",
    "        return entries\n",
    "\n",
    "    def _extract_company(self, entry: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Extract company name from entry metadata (optimized).\n",
    "        \n",
    "        Args:\n",
    "            entry (Dict): Feed entry\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted company name or empty string\n",
    "        \"\"\"\n",
    "        # Quick checks for common company information locations\n",
    "        if hasattr(entry, 'source') and entry.source.get('title'):\n",
    "            return entry.source.get('title', '')\n",
    "        \n",
    "        if hasattr(entry, 'author'):\n",
    "            return entry.author\n",
    "            \n",
    "        # Only check content if other methods fail\n",
    "        if hasattr(entry, 'content'):\n",
    "            try:\n",
    "                first_line = entry.content[0].value.split('\\n')[0]\n",
    "                if ' - ' in first_line:\n",
    "                    return first_line.split(' - ')[0].strip()\n",
    "            except (IndexError, AttributeError):\n",
    "                pass\n",
    "                \n",
    "        return ''\n",
    "\n",
    "def run2():\n",
    "    # Example usage with performance monitoring\n",
    "    feed_urls = [\n",
    "        'https://www.prnewswire.com/rss/news-releases-list.rss'\n",
    "    ]\n",
    "    \n",
    "    for url in feed_urls:\n",
    "        start_time = time.time()\n",
    "        parser = PRWireParser(url, timeout=10)  # 10 second timeout\n",
    "        \n",
    "        if parser.fetch_feed():\n",
    "            # Get latest entry\n",
    "            entries = parser.get_entries(limit=1)\n",
    "            \n",
    "            # Save to CSV with timestamp\n",
    "            if entries:\n",
    "                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f'pr_wire_news_{timestamp}.csv'\n",
    "                \n",
    "                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=entries[0].keys())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(entries)\n",
    "                    \n",
    "                print(f\"Saved {len(entries)} entries from {url} to {filename}\")\n",
    "                print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\n",
    "            else:\n",
    "                print(f\"No valid entries found for {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch feed from {url}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage with performance monitoring\n",
    "    feed_urls = [\n",
    "        'https://www.prnewswire.com/rss/news-releases-list.rss'\n",
    "    ]\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    for url in feed_urls:\n",
    "        start_time = time.time()\n",
    "        parser = PRWireParser(url, timeout=10)  # 10 second timeout\n",
    "        \n",
    "        if parser.fetch_feed():\n",
    "            # Get latest entry\n",
    "            entries = parser.get_entries(limit=1)\n",
    "            \n",
    "            if entries:\n",
    "                # Store entries in dictionary with timestamp as key\n",
    "                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                results[timestamp] = entries[0]  # Store the single entry\n",
    "                \n",
    "                print(f\"Stored entry from {url} in results dictionary\")\n",
    "                print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\n",
    "            else:\n",
    "                print(f\"No valid entries found for {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch feed from {url}\")\n",
    "    \n",
    "    return results  # Return the dictionary containing all entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "553b11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Smith', 'age': 30, 'is_student': False, 'grades': [85, 92, 78, 90], 'address': {'street': '123 Main St', 'city': 'Boston', 'state': 'MA', 'zip': '02108'}, 'courses': ('Math', 'History', 'Science'), 'gpa': 3.75}\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dictionary with different data types\n",
    "sample_dict = {\n",
    "    'name': 'John Smith',\n",
    "    'age': 30,\n",
    "    'is_student': False,\n",
    "    'grades': [85, 92, 78, 90],\n",
    "    'address': {\n",
    "        'street': '123 Main St',\n",
    "        'city': 'Boston',\n",
    "        'state': 'MA',\n",
    "        'zip': '02108'\n",
    "    },\n",
    "    'courses': ('Math', 'History', 'Science'),\n",
    "    'gpa': 3.75\n",
    "}\n",
    "\n",
    "print(sample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45dbec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed fetched in 0.16 seconds\n",
      "Stored entry from https://www.prnewswire.com/rss/news-releases-list.rss in results dictionary\n",
      "Total processing time: 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec529d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20250215_160053': {'title': 'DEAN TUCCI FILES OPPOSITION MOTION AGAINST CFPB',\n",
       "  'link': 'https://www.prnewswire.com/news-releases/dean-tucci-files-opposition-motion-against-cfpb-302377572.html',\n",
       "  'published': 'Sat, 15 Feb 2025 20:01:00 +0000',\n",
       "  'summary': '<p>PALATINE, Ill., Feb. 15, 2025 /PRNewswire/ -- In November of 2020, the CFPB filed a lawsuit against FDATR, Inc., Ken Halverson, and Dean Tucci for allegations that the Federal Telemarketing Sales Rule, 16 C.F.R. Part 310 (\"TSR\") had been violated. Dean Tucci started and owned FDATR, Inc....</p>',\n",
       "  'company': '',\n",
       "  'categories': 'LAW'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372bcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1ec33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c17c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10921c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
