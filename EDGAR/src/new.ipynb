{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 tables in the document\n",
      "FOUND INCOME STATEMENT: table_21\n",
      "FOUND BALANCE SHEET: table_23\n",
      "FOUND CASH FLOWS: table_25\n",
      "Saved: financial_tables/income_statement.csv\n",
      "Saved: financial_tables/balance_sheet.csv\n",
      "Saved: financial_tables/cash_flow_statement.csv\n",
      "\n",
      "Found 3 financial tables:\n",
      "- income_statement\n",
      "- balance_sheet\n",
      "- cash_flow_statement\n",
      "\n",
      "INCOME_STATEMENT:\n",
      "                                                                               \\\n",
      "0                     Year Ended                                                \n",
      "1                   Jan 26, 2025           Jan 28, 2024          Jan 29, 2023   \n",
      "2          Revenue             $  130,497                                   $   \n",
      "3  Cost of revenue        32,639                         16,621                 \n",
      "4     Gross profit        97,858                         44,301                 \n",
      "\n",
      "                                              \n",
      "0                                             \n",
      "1                                             \n",
      "2  60,922            $  26,974                \n",
      "3          11,618                             \n",
      "4          15,356                             \n",
      "\n",
      "BALANCE_SHEET:\n",
      "                                                                            \\\n",
      "0                             Jan 26, 2025         Jan 28, 2024              \n",
      "1                     Assets                                                 \n",
      "2            Current assets:                                                 \n",
      "3  Cash and cash equivalents             $  8,589                        $   \n",
      "4      Marketable securities        34,621                       18,704      \n",
      "\n",
      "                    \n",
      "0                   \n",
      "1                   \n",
      "2                   \n",
      "3  7,280            \n",
      "4                   \n",
      "\n",
      "CASH_FLOW_STATEMENT:\n",
      "                                                                            \\\n",
      "0                                                       Year Ended           \n",
      "1                                                     Jan 26, 2025           \n",
      "2              Cash flows from operating activities:                         \n",
      "3                                         Net income             $  72,880   \n",
      "4  Adjustments to reconcile net income to net cas...                         \n",
      "\n",
      "                                                                     \n",
      "0                                                                    \n",
      "1  Jan 28, 2024    Jan 29, 2023                                      \n",
      "2                                                                    \n",
      "3                             $  29,760      $  4,368                \n",
      "4                                                                    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_financial_tables(file_path, output_dir=\"financial_tables\"):\n",
    "    \"\"\"\n",
    "    Extract and save only the key financial tables (balance sheet, income statement, cash flow)\n",
    "    from an XBRL/HTML document.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the XBRL/HTML file\n",
    "        output_dir (str): Directory to save the CSV files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of financial DataFrames with table identifiers as keys\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Find all tables\n",
    "    tables = soup.find_all('table')\n",
    "    print(f\"Found {len(tables)} tables in the document\")\n",
    "    \n",
    "    # Dictionary to store financial DataFrames\n",
    "    financials = {}\n",
    "    \n",
    "    # Process each table\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            # Generate a table identifier\n",
    "            # Try to find a caption or a title for better identification\n",
    "            caption = table.find('caption')\n",
    "            title_element = table.find(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and tag.text.strip())\n",
    "            \n",
    "            if caption and caption.text.strip():\n",
    "                table_id = f\"table_{i}_{clean_title(caption.text.strip())}\"\n",
    "            elif title_element and title_element.text.strip():\n",
    "                table_id = f\"table_{i}_{clean_title(title_element.text.strip())}\"\n",
    "            else:\n",
    "                # Try to find nearby headings\n",
    "                prev_heading = table.find_previous(['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "                if prev_heading and prev_heading.text.strip():\n",
    "                    table_id = f\"table_{i}_{clean_title(prev_heading.text.strip())}\"\n",
    "                else:\n",
    "                    table_id = f\"table_{i}\"\n",
    "            \n",
    "            # Parse the table into a pandas DataFrame\n",
    "            df = parse_table_to_dataframe(table)\n",
    "            \n",
    "            # Skip empty tables and very small tables (likely not financial statements)\n",
    "            if df.empty or (df.shape[0] < 5 and df.shape[1] < 3):\n",
    "                continue\n",
    "                \n",
    "            # Convert DataFrame to string for keyword search\n",
    "            df_text = df.to_string().lower()\n",
    "            \n",
    "            # Check for specific financial tables using key phrases\n",
    "            if \"total liabilities and shareholders' equity\" in df_text:\n",
    "                financials[\"balance_sheet\"] = df\n",
    "                print(f\"FOUND BALANCE SHEET: {table_id}\")\n",
    "            elif \"cash and cash equivalents at end of period\" in df_text:\n",
    "                financials[\"cash_flow_statement\"] = df\n",
    "                print(f\"FOUND CASH FLOWS: {table_id}\")\n",
    "            elif \"total operating expenses\" in df_text and \"net income per share\" in df_text:\n",
    "                financials[\"income_statement\"] = df\n",
    "                print(f\"FOUND INCOME STATEMENT: {table_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing table {i}: {str(e)}\")\n",
    "    \n",
    "    # Create output directory if specified and financials found\n",
    "    if output_dir and financials:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save each financial table as CSV\n",
    "        for table_name, df in financials.items():\n",
    "            csv_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Saved: {csv_path}\")\n",
    "    \n",
    "    # Print summary of found financial tables\n",
    "    print(f\"\\nFound {len(financials)} financial tables:\")\n",
    "    for table_name in financials.keys():\n",
    "        print(f\"- {table_name}\")\n",
    "    \n",
    "    return financials\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean a title string to make it suitable for a filename or dict key\"\"\"\n",
    "    # Replace multiple spaces with a single underscore\n",
    "    title = re.sub(r'\\s+', '_', title)\n",
    "    # Remove special characters\n",
    "    title = re.sub(r'[^\\w]', '', title)\n",
    "    # Truncate long titles\n",
    "    return title[:50].lower()\n",
    "\n",
    "def parse_table_to_dataframe(table):\n",
    "    \"\"\"\n",
    "    Parse an HTML table into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        table (bs4.element.Tag): BeautifulSoup table element\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the table data\n",
    "    \"\"\"\n",
    "    # Extract headers\n",
    "    headers = []\n",
    "    header_row = table.find('tr')\n",
    "    \n",
    "    # If there's a thead, use it for headers\n",
    "    thead = table.find('thead')\n",
    "    if thead:\n",
    "        header_row = thead.find('tr')\n",
    "    \n",
    "    if header_row:\n",
    "        headers = [th.text.strip() for th in header_row.find_all(['th', 'td'])]\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    tbody = table.find('tbody')\n",
    "    if tbody:\n",
    "        # If tbody exists, get rows from there\n",
    "        table_rows = tbody.find_all('tr')\n",
    "    else:\n",
    "        # Otherwise get all rows and skip the header if it exists\n",
    "        table_rows = table.find_all('tr')\n",
    "        if headers and len(table_rows) > 0:\n",
    "            table_rows = table_rows[1:]\n",
    "    \n",
    "    for row in table_rows:\n",
    "        cells = [td.text.strip() for td in row.find_all(['td', 'th'])]\n",
    "        if cells:  # Skip empty rows\n",
    "            rows.append(cells)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if headers and rows:\n",
    "        # Make sure all rows have the same length as headers\n",
    "        for i, row in enumerate(rows):\n",
    "            if len(row) < len(headers):\n",
    "                # Pad with empty strings\n",
    "                rows[i] = row + [''] * (len(headers) - len(row))\n",
    "            elif len(row) > len(headers):\n",
    "                # Truncate\n",
    "                rows[i] = row[:len(headers)]\n",
    "                \n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "    elif rows:\n",
    "        # No headers, use generic column names\n",
    "        max_cols = max(len(row) for row in rows)\n",
    "        cols = [f'Column_{i}' for i in range(max_cols)]\n",
    "        \n",
    "        # Ensure all rows have the same length\n",
    "        for i, row in enumerate(rows):\n",
    "            if len(row) < max_cols:\n",
    "                rows[i] = row + [''] * (max_cols - len(row))\n",
    "                \n",
    "        df = pd.DataFrame(rows, columns=cols)\n",
    "    else:\n",
    "        # Empty table\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the XBRL file\n",
    "    file_path = \"../filings/sec-edgar-filings/NVDA/10-K/0001045810-25-000023/nvda_primary-document.html\"\n",
    "    \n",
    "    # Extract and save financial tables\n",
    "    financial_tables = extract_financial_tables(file_path)\n",
    "    \n",
    "    # Display the found tables\n",
    "    for table_name, df in financial_tables.items():\n",
    "        print(f\"\\n{table_name.upper()}:\")\n",
    "        print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "\n",
    "# def extract_risk_factors(file_path, output_file=None):\n",
    "#     \"\"\"\n",
    "#     Extract the Risk Factors section from an XBRL/HTML 10-K document.\n",
    "    \n",
    "#     Args:\n",
    "#         file_path (str): Path to the XBRL/HTML file\n",
    "#         output_file (str, optional): Path to save the extracted text\n",
    "        \n",
    "#     Returns:\n",
    "#         str: The extracted Risk Factors text\n",
    "#     \"\"\"\n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(file_path):\n",
    "#         raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "#     # Read the file\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "    \n",
    "#     # Parse the HTML content\n",
    "#     soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "#     # Search for the Risk Factors section\n",
    "#     # We'll try different approaches to find it\n",
    "    \n",
    "#     # First approach: Look for heading elements containing the text\n",
    "#     risk_start = None\n",
    "#     risk_end = None\n",
    "    \n",
    "#     # Try to find the section headings (they might be in different heading levels)\n",
    "#     for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'p']):\n",
    "#         text = heading.get_text().strip()\n",
    "        \n",
    "#         # Look for the start of the Risk Factors section\n",
    "#         if re.search(r'item\\s+1a\\.?\\s+risk\\s+factors', text, re.IGNORECASE):\n",
    "#             risk_start = heading\n",
    "#             print(f\"Found Risk Factors section start: {text}\")\n",
    "            \n",
    "#         # Look for the start of the next section\n",
    "#         if risk_start and re.search(r'item\\s+1b\\.?\\s+unresolved\\s+staff\\s+comments', text, re.IGNORECASE):\n",
    "#             risk_end = heading\n",
    "#             print(f\"Found next section start: {text}\")\n",
    "#             break\n",
    "    \n",
    "#     # If we found both the start and end points\n",
    "#     if risk_start and risk_end:\n",
    "#         # Method 1: Extract all elements between start and end\n",
    "#         risk_text = \"\"\n",
    "#         current = risk_start.next_element\n",
    "        \n",
    "#         while current and current != risk_end:\n",
    "#             if hasattr(current, 'get_text'):\n",
    "#                 text = current.get_text().strip()\n",
    "#                 if text:\n",
    "#                     risk_text += text + \"\\n\\n\"\n",
    "#             # Move to the next element\n",
    "#             try:\n",
    "#                 current = current.next_element\n",
    "#             except AttributeError:\n",
    "#                 break\n",
    "                \n",
    "#     else:\n",
    "#         # Alternative method: Try to find the sections by text search\n",
    "#         print(\"Trying alternative method: text search\")\n",
    "        \n",
    "#         # Find the Risk Factors section by text search\n",
    "#         risk_pattern = re.compile(r'item\\s+1a\\.?\\s+risk\\s+factors', re.IGNORECASE)\n",
    "#         next_section_pattern = re.compile(r'item\\s+1b\\.?\\s+unresolved\\s+staff\\s+comments', re.IGNORECASE)\n",
    "        \n",
    "#         # Get the full text\n",
    "#         full_text = soup.get_text()\n",
    "        \n",
    "#         # Find the indices of the sections\n",
    "#         matches = list(risk_pattern.finditer(full_text))\n",
    "#         if matches:\n",
    "#             start_idx = matches[0].start()\n",
    "            \n",
    "#             # Find the next section\n",
    "#             next_matches = list(next_section_pattern.finditer(full_text))\n",
    "#             if next_matches:\n",
    "#                 end_idx = next_matches[0].start()\n",
    "                \n",
    "#                 # Extract the text between the two sections\n",
    "#                 risk_text = full_text[start_idx:end_idx].strip()\n",
    "#                 print(f\"Found Risk Factors section via text search: {len(risk_text)} characters\")\n",
    "#             else:\n",
    "#                 risk_text = \"Next section marker not found.\"\n",
    "#         else:\n",
    "#             risk_text = \"Risk Factors section not found.\"\n",
    "    \n",
    "#     # Clean up the text\n",
    "#     # Remove excessive whitespace and normalize line breaks\n",
    "#     risk_text = re.sub(r'\\n\\s*\\n', '\\n\\n', risk_text)\n",
    "#     risk_text = re.sub(r' +', ' ', risk_text)\n",
    "    \n",
    "#     # Save to file if requested\n",
    "#     if output_file and risk_text:\n",
    "#         with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "#             out_file.write(risk_text)\n",
    "#         print(f\"Saved Risk Factors to: {output_file}\")\n",
    "    \n",
    "#     return risk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Path to the XBRL file\n",
    "#     file_path = \"../filings/sec-edgar-filings/NVDA/10-K/0001045810-25-000023/nvda_primary-document.html\"\n",
    "    \n",
    "#     # Output file path\n",
    "#     output_file = \"nvidia_risk_factors.txt\"\n",
    "    \n",
    "#     # Extract risk factors\n",
    "#     risk_factors = extract_risk_factors(file_path, output_file)\n",
    "    \n",
    "#     # Print the first 500 characters as a preview\n",
    "#     print(\"\\nRISK FACTORS PREVIEW:\")\n",
    "#     # print(risk_factors[:500] + \"...\")\n",
    "#     print(risk_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_risk_factors(file_path, output_file=None):\n",
    "#     \"\"\"\n",
    "#     Extract the Risk Factors section from an XBRL/HTML 10-K document.\n",
    "    \n",
    "#     Args:\n",
    "#         file_path (str): Path to the XBRL/HTML file\n",
    "#         output_file (str, optional): Path to save the extracted text\n",
    "        \n",
    "#     Returns:\n",
    "#         str: The extracted Risk Factors text\n",
    "#     \"\"\"\n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(file_path):\n",
    "#         raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "#     # Read the file\n",
    "#     with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "#         content = file.read()\n",
    "    \n",
    "#     # Parse the HTML content\n",
    "#     soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "#     # Remove script and style elements that might interfere with text extraction\n",
    "#     for script in soup(['script', 'style']):\n",
    "#         script.decompose()\n",
    "    \n",
    "#     # First, try to find the section by looking for specific headers or section markers\n",
    "#     risk_section_pattern = re.compile(r'item\\s+1a\\.?\\s+risk\\s+factors', re.IGNORECASE)\n",
    "#     next_section_pattern = re.compile(r'item\\s+1b\\.?\\s+unresolved\\s+staff\\s+comments', re.IGNORECASE)\n",
    "    \n",
    "#     # Get all text elements\n",
    "#     text_elements = []\n",
    "#     for tag in soup.find_all(['div', 'p', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "#         if tag.get_text().strip():\n",
    "#             text_elements.append({\n",
    "#                 'element': tag,\n",
    "#                 'text': tag.get_text().strip()\n",
    "#             })\n",
    "    \n",
    "#     # Find the Risk Factors section and the next section\n",
    "#     risk_start_idx = None\n",
    "#     risk_end_idx = None\n",
    "    \n",
    "#     for idx, elem in enumerate(text_elements):\n",
    "#         if risk_section_pattern.search(elem['text']) and risk_start_idx is None:\n",
    "#             risk_start_idx = idx\n",
    "#             print(f\"Found Risk Factors section: {elem['text']}\")\n",
    "        \n",
    "#         if risk_start_idx is not None and next_section_pattern.search(elem['text']):\n",
    "#             risk_end_idx = idx\n",
    "#             print(f\"Found next section: {elem['text']}\")\n",
    "#             break\n",
    "    \n",
    "#     # Extract the risk factors content\n",
    "#     risk_factors_text = \"\"\n",
    "    \n",
    "#     if risk_start_idx is not None and risk_end_idx is not None:\n",
    "#         # Extract text between the start and end sections\n",
    "#         for idx in range(risk_start_idx, risk_end_idx):\n",
    "#             text = text_elements[idx]['text']\n",
    "#             if text and not re.match(r'^Table of Contents$', text):\n",
    "#                 risk_factors_text += text + \"\\n\\n\"\n",
    "#     else:\n",
    "#         # Alternative method: Try full text search\n",
    "#         print(\"Using alternative method: full text search\")\n",
    "#         full_text = soup.get_text()\n",
    "        \n",
    "#         matches = list(risk_section_pattern.finditer(full_text))\n",
    "#         next_matches = list(next_section_pattern.finditer(full_text))\n",
    "        \n",
    "#         if matches and next_matches:\n",
    "#             start_idx = matches[0].start()\n",
    "#             end_idx = next_matches[0].start()\n",
    "#             risk_factors_text = full_text[start_idx:end_idx].strip()\n",
    "    \n",
    "#     # Clean up the text\n",
    "#     # Remove page numbers\n",
    "#     risk_factors_text = re.sub(r'\\n\\d+\\n', '\\n', risk_factors_text)\n",
    "    \n",
    "#     # Remove \"Table of Contents\" references\n",
    "#     risk_factors_text = re.sub(r'Table of Contents', '', risk_factors_text)\n",
    "    \n",
    "#     # Remove excessive whitespace and normalize line breaks\n",
    "#     risk_factors_text = re.sub(r'\\n\\s*\\n', '\\n\\n', risk_factors_text)\n",
    "#     risk_factors_text = re.sub(r' +', ' ', risk_factors_text)\n",
    "    \n",
    "#     # Extract risk factors summary and categories\n",
    "#     # Structure the output with headers and bullet points\n",
    "#     structured_output = \"\"\n",
    "    \n",
    "#     # Add the title\n",
    "#     if \"Item 1A. Risk Factors\" in risk_factors_text:\n",
    "#         structured_output += \"Item 1A. Risk Factors\\n\\n\"\n",
    "    \n",
    "#     # Process and add the content with better formatting\n",
    "#     content_lines = risk_factors_text.split('\\n')\n",
    "#     current_section = \"\"\n",
    "    \n",
    "#     for line in content_lines:\n",
    "#         line = line.strip()\n",
    "        \n",
    "#         # Skip empty lines and page numbers\n",
    "#         if not line or re.match(r'^\\d+$', line):\n",
    "#             continue\n",
    "            \n",
    "#         # Check if this is a new section heading\n",
    "#         if \"Risk Factors Summary\" in line:\n",
    "#             structured_output += \"Risk Factors Summary\\n\\n\"\n",
    "#             current_section = \"summary\"\n",
    "#         elif re.search(r'Risks Related to', line):\n",
    "#             structured_output += f\"\\n{line}\\n\\n\"\n",
    "#             current_section = \"category\"\n",
    "#         elif line.startswith(\"•\") or line.startswith(\"-\"):\n",
    "#             structured_output += f\"{line}\\n\"\n",
    "#         else:\n",
    "#             structured_output += f\"{line}\\n\"\n",
    "    \n",
    "#     # Save to file if requested\n",
    "#     if output_file and structured_output:\n",
    "#         with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "#             out_file.write(structured_output)\n",
    "#         print(f\"Saved Risk Factors to: {output_file}\")\n",
    "    \n",
    "#     return structured_output\n",
    "\n",
    "# # Function to clean HTML and extract plain text\n",
    "# def extract_plain_text_from_html(html_content):\n",
    "#     \"\"\"\n",
    "#     Extracts plain text from HTML content, preserving structure.\n",
    "    \n",
    "#     Args:\n",
    "#         html_content (str): HTML content\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Plain text\n",
    "#     \"\"\"\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "#     # Remove script and style elements\n",
    "#     for script in soup(['script', 'style']):\n",
    "#         script.decompose()\n",
    "    \n",
    "#     # Process the HTML structure\n",
    "#     lines = []\n",
    "    \n",
    "#     # Process headings, paragraphs, list items\n",
    "#     for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'div']):\n",
    "#         text = tag.get_text(strip=True)\n",
    "#         if text:\n",
    "#             if tag.name.startswith('h'):\n",
    "#                 lines.append(f\"\\n{text}\\n\")\n",
    "#             elif tag.name == 'li':\n",
    "#                 lines.append(f\"• {text}\")\n",
    "#             else:\n",
    "#                 lines.append(text)\n",
    "    \n",
    "#     return '\\n'.join(lines)\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Path to the XBRL file\n",
    "#     file_path = \"../filings/sec-edgar-filings/NVDA/10-K/0001045810-25-000023/nvda_primary-document.html\"\n",
    "    \n",
    "#     # Output file path\n",
    "#     output_file = \"nvidia_risk_factors.txt\"\n",
    "    \n",
    "#     # Extract risk factors\n",
    "#     risk_factors = extract_risk_factors(file_path, output_file)\n",
    "    \n",
    "#     # Print the first 1000 characters as a preview\n",
    "#     print(\"\\nRISK FACTORS PREVIEW:\")\n",
    "#     print(risk_factors[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 125813 characters of Risk Factors text\n",
      "Saved Risk Factors to: nvidia_risk_factors.txt\n",
      "\n",
      "RISK FACTORS PREVIEW:\n",
      "Item 1A. Risk Factors – Risks Related to Regulatory, Legal, Our Stock and Other Matters” for a discussion of this potential impact.10Compliance with laws, rules, and regulations has not otherwise had a material effect upon our capital expenditures, results of operations, or competitive position and we do not currently anticipate material capital expenditures for environmental control facilities. Compliance with existing or future governmental regulations, including, but not limited to, those per...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_risk_factors(file_path, output_file=None):\n",
    "    \"\"\"\n",
    "    Extract the Risk Factors section from an XBRL/HTML 10-K document.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the XBRL/HTML file\n",
    "        output_file (str, optional): Path to save the extracted text\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted Risk Factors text\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Define patterns for finding the Risk Factors section\n",
    "    risk_start_pattern = re.compile(r'Item\\s+1A\\.?\\s+Risk\\s+Factors', re.IGNORECASE)\n",
    "    next_section_pattern = re.compile(r'Item\\s+1B\\.?\\s+Unresolved\\s+Staff\\s+Comments', re.IGNORECASE)\n",
    "    \n",
    "    # Get the full text\n",
    "    full_text = soup.get_text()\n",
    "    \n",
    "    # Find the start and end positions of the Risk Factors section\n",
    "    risk_start_matches = list(risk_start_pattern.finditer(full_text))\n",
    "    next_section_matches = list(next_section_pattern.finditer(full_text))\n",
    "    \n",
    "    if not risk_start_matches or not next_section_matches:\n",
    "        print(\"Could not find the Risk Factors section or next section.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Get the text between the start and end positions\n",
    "    risk_start_pos = risk_start_matches[0].start()\n",
    "    next_section_pos = next_section_matches[0].start()\n",
    "    \n",
    "    risk_factors_text = full_text[risk_start_pos:next_section_pos].strip()\n",
    "    print(f\"Extracted {len(risk_factors_text)} characters of Risk Factors text\")\n",
    "    \n",
    "    # Clean up the text\n",
    "    # Remove page numbers\n",
    "    risk_factors_text = re.sub(r'\\n\\d+\\n', '\\n', risk_factors_text)\n",
    "    \n",
    "    # Remove \"Table of Contents\" references\n",
    "    risk_factors_text = re.sub(r'Table of Contents', '', risk_factors_text)\n",
    "    \n",
    "    # Remove excessive whitespace and normalize line breaks\n",
    "    risk_factors_text = re.sub(r'\\n\\s*\\n', '\\n\\n', risk_factors_text)\n",
    "    risk_factors_text = re.sub(r' +', ' ', risk_factors_text)\n",
    "    \n",
    "    # Deduplicate the content\n",
    "    content_lines = risk_factors_text.split('\\n')\n",
    "    unique_lines = []\n",
    "    seen_lines = set()\n",
    "    \n",
    "    for line in content_lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines and already seen lines\n",
    "        if not line or line in seen_lines:\n",
    "            continue\n",
    "        seen_lines.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    deduped_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            out_file.write(deduped_text)\n",
    "        print(f\"Saved Risk Factors to: {output_file}\")\n",
    "    \n",
    "    return deduped_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the XBRL file\n",
    "    file_path = \"../filings/sec-edgar-filings/NVDA/10-K/0001045810-25-000023/nvda_primary-document.html\"\n",
    "    \n",
    "    # Output file path\n",
    "    output_file = \"nvidia_risk_factors.txt\"\n",
    "    \n",
    "    # Extract risk factors\n",
    "    risk_factors = extract_risk_factors(file_path, output_file)\n",
    "    \n",
    "    # Print the first 500 characters as a preview\n",
    "    if risk_factors:\n",
    "        print(\"\\nRISK FACTORS PREVIEW:\")\n",
    "        print(risk_factors[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rss-kernel",
   "language": "python",
   "name": "rss-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
